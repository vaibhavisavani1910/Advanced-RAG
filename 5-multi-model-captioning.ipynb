{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a5f6c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF for PDF parsing\n",
    "import io\n",
    "from PIL import Image\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import MongoDBAtlasVectorSearch\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "google_api_key = os.environ.get(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1af7917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf_with_images(pdf_path, output_folder=\"pdf_images\"):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text_content = []\n",
    "\n",
    "    for page_num, page in enumerate(doc):\n",
    "        # Extract text\n",
    "        text_content.append(page.get_text(\"text\"))\n",
    "\n",
    "        # Extract images\n",
    "        for img_index, img in enumerate(page.get_images(full=True)):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "\n",
    "            # Save image to disk\n",
    "            img_pil = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "            img_filename = os.path.join(output_folder, f\"page_{page_num}_img_{img_index}.png\")\n",
    "            img_pil.save(img_filename)\n",
    "\n",
    "    return \"\\n\".join(text_content), output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0698a875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = init_chat_model(\"google_genai:gemini-2.5-flash\")\n",
    "\n",
    "def caption_images(image_folder=\"extracted_images\"):\n",
    "    img_data = []\n",
    "\n",
    "    for img_file in sorted(os.listdir(image_folder)):\n",
    "        if img_file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            img_path = os.path.join(image_folder, img_file)\n",
    "\n",
    "            # Open image and convert to bytes\n",
    "            with open(img_path, \"rb\") as f:\n",
    "                img_bytes = f.read()\n",
    "\n",
    "            # Encode image as base64\n",
    "            img_b64 = base64.b64encode(img_bytes).decode(\"utf-8\")\n",
    "\n",
    "            # Create message for LLM\n",
    "            message = {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": (\n",
    "                            \"You are an assistant tasked with summarizing tables, images and text \"\n",
    "                            \"for retrieval. These summaries will be embedded and used to retrieve \"\n",
    "                            \"the raw text or table elements. Give a concise summary optimized for retrieval.\"\n",
    "                        ),\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"file\",\n",
    "                        \"source_type\": \"base64\",\n",
    "                        \"data\": img_b64,\n",
    "                        \"mime_type\": \"image/jpeg\",  # or \"image/png\"\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "\n",
    "            # Invoke the model\n",
    "            response = llm.invoke([message])\n",
    "\n",
    "            img_data.append({\n",
    "                \"response\": response.text(),\n",
    "                \"name\": img_file\n",
    "            })\n",
    "\n",
    "    return img_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5eaadef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_documents(text, image_captions):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    text_docs = splitter.create_documents([text])\n",
    "    img_docs = splitter.create_documents(image_captions)\n",
    "    return text_docs + img_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ee3bc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_in_vectorstore(docs):\n",
    "    client = MongoClient(os.getenv(\"MONGODB_URI\"))\n",
    "    collection = client[\"RAG-evaluation\"][\"RAG-multimodel\"]\n",
    "\n",
    "    embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "    vectorstore = MongoDBAtlasVectorSearch.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embedding,\n",
    "        collection=collection,\n",
    "        index_name=\"default\"\n",
    "    )\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8717ba66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 39512 characters of text and 10 images.\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"/Users/vaibhavisavani/Desktop/Gen-AI/Advanced-RAG/data/attention_is_all_you_need.pdf\"  \n",
    "text, image_list = read_pdf_with_images(pdf_path)\n",
    "\n",
    "print(f\"Extracted {len(text)} characters of text and {len(image_list)} images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65bb4393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pdf_images'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df08f13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Image Captions: [{'response': 'Diagram of the Transformer model architecture, depicting an encoder-decoder structure. The encoder processes inputs using input embedding, positional encoding, multi-head attention, feed forward networks, and Add & Norm layers. The decoder processes shifted outputs using output embedding, positional encoding, masked multi-head attention, cross-attention (Multi-Head Attention), feed forward networks, and Add & Norm layers, culminating in output probabilities via Linear and Softmax layers.', 'name': 'page_2_img_0.png'}, {'response': 'A computational diagram depicting the Scaled Dot-Product Attention mechanism. It shows inputs Q, K, and V, followed by matrix multiplication of Q and K, scaling, optional masking, softmax activation, and a final matrix multiplication with V.', 'name': 'page_3_img_0.png'}, {'response': 'This image illustrates the Multi-Head Attention mechanism, a core component of Transformer models. It shows inputs V (Values), K (Keys), and Q (Queries) each undergoing linear transformations. These transformed inputs then feed into multiple \"Scaled Dot-Product Attention\" heads. The outputs from these attention heads are then concatenated, followed by a final linear transformation to produce the output.', 'name': 'page_3_img_1.png'}]\n"
     ]
    }
   ],
   "source": [
    "# Caption images\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "image_captions = caption_images(image_list)\n",
    "print(\"Generated Image Captions:\", image_captions)  # preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15c7041c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Diagram of the Transformer model architecture, depicting an encoder-decoder structure. The encoder processes inputs using input embedding, positional encoding, multi-head attention, feed forward networks, and Add & Norm layers. The decoder processes shifted outputs using output embedding, positional encoding, masked multi-head attention, cross-attention (Multi-Head Attention), feed forward networks, and Add & Norm layers, culminating in output probabilities via Linear and Softmax layers.',\n",
       " 'A computational diagram depicting the Scaled Dot-Product Attention mechanism. It shows inputs Q, K, and V, followed by matrix multiplication of Q and K, scaling, optional masking, softmax activation, and a final matrix multiplication with V.',\n",
       " 'This image illustrates the Multi-Head Attention mechanism, a core component of Transformer models. It shows inputs V (Values), K (Keys), and Q (Queries) each undergoing linear transformations. These transformed inputs then feed into multiple \"Scaled Dot-Product Attention\" heads. The outputs from these attention heads are then concatenated, followed by a final linear transformation to produce the output.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions = [image_caption['response'] for image_caption in image_captions]\n",
    "captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d26c236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare docs\n",
    "docs = prepare_documents(text, captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28504274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s2/tb48c2wd03jbyvkwy3zmp8gw0000gn/T/ipykernel_48215/718355382.py:5: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multimodal vector store created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Store in MongoDB\n",
    "vectorstore = store_in_vectorstore(docs)\n",
    "print(\"Multimodal vector store created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0fc0f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8868dc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved: Attention(Q, K, V ) = softmax(QKT\n",
      "√dk\n",
      ")V\n",
      "(1)\n",
      "The two most commonly used attention functions are additive attention [2], and dot-product (multi-\n",
      "plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
      "of\n",
      "1\n",
      "√dk . Additive attention computes the compatibility function using a feed-forward network with\n",
      "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
      "Retrieved: reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "Retrieved: Scaled Dot-Product Attention\n",
      "Multi-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "attention layers running in parallel.\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
      "query with the corresponding key.\n",
      "3.2.1\n",
      "Scaled Dot-Product Attention\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      "Retrieved: This image illustrates the Multi-Head Attention mechanism, a core component of Transformer models. It shows inputs V (Values), K (Keys), and Q (Queries) each undergoing linear transformations. These transformed inputs then feed into multiple \"Scaled Dot-Product Attention\" heads. The outputs from these attention heads are then concatenated, followed by a final linear transformation to produce the output.\n",
      "Retrieved: masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
      "predictions for position i can depend only on the known outputs at positions less than i.\n",
      "3.2\n",
      "Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "query = \"How does attention work\"\n",
    "results = retriever.get_relevant_documents(query)\n",
    "\n",
    "for r in results:\n",
    "    print(\"Retrieved:\", r.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1161f38a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
